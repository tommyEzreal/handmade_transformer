# handmade_transformer ğŸ¤–
> âš ï¸ my own implementation of transformer architecture      
> try to implement "Attention is all you need(2017)" in low_level as possible    
> original paper : https://arxiv.org/abs/1706.03762 
>             
> 23.01.28 ~ working on ğŸ’ªğŸ»


<br><br>
## Requirements
```
torch==1.13.1
spacy==3.4.4
soynlp==0.0.493
torchtext==0.6.0
en-core-web-sm==2.1.0
```

<br><br>
## Multi-Head Attention
<img src = "https://user-images.githubusercontent.com/100064247/218194035-e14245fb-f10d-43ab-8f80-88cb0c6dd78d.png" width=50% height=50% align="center" >


```python
class MultiHeadAttention(nn.Module):
  def __init__(self, embed_size, heads, dropout=0.1): 
    # embed_size = d_model, heads = num_heads , head_dim = d_k
    super(MultiHeadAttention,self).__init__()

    self.embed_size = embed_size # d_model  (ì„ë² ë”©ì°¨ì› , hidden_dim)
    self.heads = heads # num_heads
    self.head_dim = embed_size // heads # dim of each head 
    assert (self.head_dim * heads == embed_size), "Embed size needs to be div by heads"

    self.fc_queries = nn.Linear(embed_size, embed_size) # fc layer of Query
    self.fc_keys = nn.Linear(embed_size, embed_size) # fc Key
    self.fc_values = nn.Linear(embed_size, embed_size) # fc Value

    self.dropout = nn.Dropout(dropout)
    self.fc_out = nn.Linear(embed_size, embed_size)

  def forward(self, query, key, value, mask=None):
    N = query.shape[0] # N = batch
      
    
    # query = [batch_size, query_len, embed_size]
    # key = [batch_size, key_len, embed_size]
    # value = [batch_size, value_len, embed_size]
    

    # headê°œìˆ˜ë§Œí¼ ë‚˜ëˆ ì£¼ê¸° (num_headsë§Œí¼ì˜ K,Q,V)
    # view: [batch, qkv_len, embed_size] -> [batch, qkvlen, n_heads, head_dim]
    query = self.fc_queries(query).view(N , -1, self.heads, self.head_dim)
    key = self.fc_keys(key).view(N, -1, self.heads, self.head_dim)
    value = self.fc_values(value).view(N, -1, self.heads, self.head_dim)

    # = permute(0,2,1,3) = (batch(N), num_heads(self.heads), qkv_len, self.head_dim)
    query = query.transpose(1,2) # [batch, n_heads, query_len, head_dim]
    key = key.transpose(1,2) # [batch, n_heads, key_len, head_dim]
    value = value.transpose(1,2) # [batch, n_heads, value_len, head_dim]


    # dot_product attention calculate
    scaled_dp_attn = self.dot_product_attention(query, key, value, self.head_dim, mask, self.dropout)
    # scaled_dot_product_attention : [batch, n_heads, q_len, head_dim]

    concat = scaled_dp_attn.transpose(1,2).contiguous() # [batch, q_len, n_heads, head_dim]
    concat = concat.view(N, -1, self.embed_size) # ë‹¤ì‹œ embedsizeë¡œ concat [batch, q_len, n_heads*head_dim]
    out = self.fc_out(concat) # fc layer in 

    return out # [batch_size, query_len, embed_size]

  # ë‚´ì ì—°ì‚°ë¶€ë¶„ ë”°ë¡œ ì •ì˜ 
  def dot_product_attention(self, query, key, value, head_dim, mask=None, dropout=None):
    energy = torch.matmul(query, key.transpose(-2,-1) / math.sqrt(self.head_dim)) # attn energy
    # key.tanspose(-2,-1): [batch, n_heads, key_len, head_dim] -> [batch, n_heads, head_dim, key_len]
    # torch.matmul(q,k.transpose(-2,-1)) : [b,n_h,q_len,h_d] * [b,n_h,h_d,k_len] = [b, n_h, q_len, k_len]
    # /math.sqrt(self.head_dim) = scaling with head_dim  before softmax

    if mask is not None: # mask (optional)
      energy = energy.masked_fill(mask==0, -1e20)

    attention = torch.nn.functional.softmax(energy, dim=-1) # attention score  
    # [batch, n_heads, q_len, k_len]

    if dropout is not None:
      attention = dropout(attention)

    # softamx(QKtrans/scale) dot value
    # softmax( [batch, n_heads, q_len, k_len] * [batch, n_heads, v_len, head_dim] )
    out = torch.matmul(attention, value) # out: [batch, n_heads, q_len, head_dim]
    return out # scaled_dot_product_attention

```

<br><br>
## PositionWise FeedForward
<img width="700" alt="image" src="https://user-images.githubusercontent.com/100064247/218197012-a6128a5a-3575-4766-ad70-2118dcee85e6.png">


```python
class FeedForward(nn.Module):
  def __init__(self, embed_size, forward_expansion, dropout):
    super(FeedForward,self).__init__()

    self.fc_1 = nn.Linear(embed_size, forward_expansion * embed_size)
    self.fc_2 = nn.Linear(forward_expansion * embed_size, embed_size)
    self.relu = nn.ReLU()
    self.dropout = nn.Dropout(dropout)

  def forward(self, x): # x: [batch, seq_len, embed_size]
    x = self.fc_1(x) # [batch, seq_len, forward_expansion * embed_size]
    x = self.relu(x) 
    x = self.dropout(x)
    x = self.fc_2(x) # [batch, seq_len, embed_size ]
    return x 
```
<br><br>
## Layer Normalization
<img width="807" alt="image" src="https://user-images.githubusercontent.com/100064247/219100644-d2b1da0c-d94b-4524-a729-b1ea954a3ce5.png">      
<img width="810" alt="image" src="https://user-images.githubusercontent.com/100064247/219100728-cb35e6a3-bb1c-4935-857c-0a9f3a7b06bc.png">      

> https://arxiv.org/abs/1607.06450
```python
class LayerNormalization(nn.Module):
  def __init__(self, embed_size, epsilon=1e-9):
    super(LayerNormalization, self).__init__()
    
    self.epsilon = epsilon 
    self.gamma = nn.Parameter(torch.ones(embed_size))
    self.beta = nn.Parameter(torch.zeros(embed_size))

  def forward(self, x):
    # mean/std of each layer 
    mean = x.mean(dim=-1, keepdim=True)
    var = ((x-mean)**2).mean(dim=-1, keepdim=True)
    std = (var+self.epsilon).sqrt() # eps: prevent from div to 0 

    y = (x-mean) / std
    out = self.gamma * y + self.beta # make learnable 
    return out 

```

<br><br>
## Positional Encoding
<img width="810" alt="image" src="https://user-images.githubusercontent.com/100064247/219105515-55b41343-577d-4c3d-ab68-fb7265ecc995.png">        


```python
class PositionalEncoding(nn.Module):
  def __init__(self,  max_len, embed_size, dropout=0.1):
    super().__init__()

    self.embed_size = embed_size
    self.dropout = dropout
    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    self.pe = torch.zeros(max_len, embed_size).to(self.device)
    self.pe.requires_grad=False #

    pos = torch.arange(0, max_len).unsqueeze(1).float()
    
    two_i = torch.arange(0, embed_size, step=2).float()
    div_term = torch.pow(10000, (two_i/torch.Tensor([embed_size]))).float()
    
    self.pe[:, 0::2] = torch.sin(pos/div_term) # even = sin 
    self.pe[:, 1::2] = torch.cos(pos/div_term) # odd = cos 

    # self.pe = self.pe.unsqueeze(0) # [batch, max_len, embed_size]


  # x input from Embedding 
  def forward(self, x): # x = src: src_sent from Transformer -> embedding 
    batch_size, seq_len = x.shape # 

  
    return self.pe[:seq_len, :]
```
   ### Label Smoothing     
   ![image](https://user-images.githubusercontent.com/100064247/223716969-4109345f-4cdd-4fd6-9606-aa123c70257d.png)    
   > working on..




<br><br>
## Encoder
> transformer block = encoder_block 
![image](https://user-images.githubusercontent.com/100064247/218197650-da8be027-5a7f-4e59-a482-cb5e50a45cea.png)



```python
class TransformerBlock(nn.Module):
  def __init__(self, embed_size, heads, forward_expansion, dropout):
    super(TransformerBlock, self).__init__()

    self.attention = MultiHeadAttention(embed_size, heads)
    self.attn_layer_norm = LayerNormalization(embed_size)
    self.ff_layer_norm = LayerNormalization(embed_size)
    self.feed_forward = FeedForward(embed_size,forward_expansion,dropout)
    self.dropout = nn.Dropout(dropout)
  
  # í•˜ë‚˜ì˜ src ì„ë² ë”©ì„ Q,K,Vë¡œ ë³µì œí•˜ì—¬ ì…ë ¥ 
  def forward(self, src, src_mask):
    # attention block & attn_layer_norm
    _src = self.attention(query = src, key = src, value = src, mask = src_mask)
    src = self.attn_layer_norm(src + self.dropout(_src)) # src ê·¸ëŒ€ë¡œì™€ attention í†µê³¼í•œ _src ë‘˜ë‹¤ layer_norm í†µê³¼ 
    
    # feed_forward block & ff_layer_norm
    _src =self.feed_forward(src)
    src = self.ff_layer_norm(src + self.dropout(_src))    
    
    return src
```

```python
class Encoder(nn.Module):
  def __init__(self,
             src_vocab_size, # input_dim (í•˜ë‚˜ì˜ ë‹¨ì–´ì— ëŒ€í•œ ì›í•« ì¸ì½”ë”©ì˜ ì°¨ì›)
             embed_size, # hidden_dim (í•˜ë‚˜ì˜ ë‹¨ì–´ì— ëŒ€í•œ ì„ë² ë”© ì°¨ì›)
             num_layers, # num_encoder_layer 
             heads, # num_heads 
             forward_expansion, #feedforward dim / embed_size
             dropout,
             device,
             max_length):
    super(Encoder, self).__init__()
    
    self.device = device

    self.embed_size = embed_size

    self.word_embedding = nn.Embedding(src_vocab_size, embed_size)
    self.position_encoding = PositionalEncoding(max_length, embed_size)

    self.layers = nn.ModuleList(
        [
            TransformerBlock(
                embed_size,
                heads, 
                forward_expansion = forward_expansion,
                dropout = dropout
            )
        for _ in range(num_layers)] # encoder layer ìˆ˜ë§Œí¼ ë°˜ë³µ 
    ) 
    self.dropout = nn.Dropout(dropout)

  def forward(self, src , src_mask):
    # src, src_mask: [batch, src_len]

    # out = src embedding + positional encoding 
    # for each batch, same positional encoding will be applied 
    out = self.dropout(self.word_embedding(src) + self.position_encoding(src))
    
    # ê° encoder layerë§ˆë‹¤ ìˆ˜í–‰  
    for layer in self.layers:
      out = layer(out, src_mask) # ê° layerì˜ outì´ ë‹¤ì‹œ ë‹¤ìŒ Layerì˜ input(src)ìœ¼ë¡œ
    # out: [batch, src_len, embed_size]

    return out # ë§ˆì§€ë§‰ encoder layerì˜ ì¶œë ¥ 
```
<br><br>
## Decoder
![image](https://user-images.githubusercontent.com/100064247/218197876-f5e64f84-71c2-4fb9-b9fa-f5b794dd979a.png)


```python
class DecoderBlock(nn.Module):
  def __init__(self, embed_size, heads, forward_expansion, dropout, device):
    super(DecoderBlock, self).__init__()

    # ì¤‘ëµí•´ì„œë„ ê°€ëŠ¥í•¨ 
    # self.attention = MultiHeadAttention(embed_size, heads)
    # self.norm = nn.LayerNorm(embed_size)
    # self.transformer_block = TransformerBlock(
    #     embed_size, heads, dropout, forward_expansion
    # )
    # self.dropout = nn.Dropout(dropout)

    self.self_attention = MultiHeadAttention(embed_size, heads)
    self.encoder_attention = MultiHeadAttention(embed_size, heads)

    self.ff_layer_norm = LayerNormalization(embed_size)
    self.attn_layer_norm = LayerNormalization(embed_size)
    self.enc_layer_norm = LayerNormalization(embed_size)

    self.feed_forward = FeedForward(embed_size, forward_expansion, dropout)

    self.dropout = nn.Dropout(dropout)

  def forward(self, trg, enc_out, trg_mask, src_mask):
    
    # trg: [batch, trg_len, embed_size]
    # enc_out: [batch, src_len, embed_size]
    # trg_mask : [batch, trg_len]
    # src_mask : [batch, src_len]

    # self attention block
    _trg = self.self_attention(trg,trg,trg,trg_mask)
    
    # LayerNorm (_trg + trgë‘˜ë‹¤ ì…ë ¥ìœ¼ë¡œ) 
    trg = self.attn_layer_norm(trg+self.dropout(_trg))
    # trg : [batch, trg_len, embed_size]

    # encoder attention block
    # Queryë§Œ decoderì…ë ¥ì¸ trgë¡œ ë°›ì•„ì˜¤ê³  Key, Valueë¥¼ encoderì˜ ë§ˆì§€ë§‰ layerë¡œ ë¶€í„° ë°›ì€ enc_out 
    _trg = self.encoder_attention(trg, enc_out, enc_out, src_mask)

    # LayerNorm()
    trg = self.enc_layer_norm(trg + self.dropout(_trg))
    # trg size same 

    # feed_forward block
    _trg = self.feed_forward(trg)

    # LayerNorm()
    trg = self.ff_layer_norm(trg + self.dropout(_trg))
    # trg size same 

    return trg
```

```python
class Decoder(nn.Module):
  def __init__(self,
               trg_vocab_size,
               embed_size,
               num_layers,
               heads,
               forward_expansion,
               dropout,
               device,
               max_length):
    super(Decoder, self).__init__()

    self.device = device
    self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)
    self.position_encoding = PositionalEncoding(max_length, embed_size)
    
    self.layers = nn.ModuleList(
        [DecoderBlock(embed_size, heads, forward_expansion, dropout, device)
        for _ in range(num_layers)]
    )
    self.fc_out = nn.Linear(embed_size, trg_vocab_size)
    self.dropout = nn.Dropout(dropout)

  def forward(self, trg, enc_out, trg_mask, src_mask):
    # trg: [batch, trg_len]
    # enc_out: [batch, src_len, embed_size]
    # trg_mask: [batch, trg_len]
    # src_mask: [batch, src_len]

    trg = self.dropout((self.word_embedding(trg))+ self.position_encoding(trg))
    # trg: [batch, trg_len, embed_size]

    for layer in self.layers:
      trg = layer(trg, enc_out, trg_mask, src_mask) # src, trg mask ëª¨ë‘ ì‚¬ìš© 
    
    out = self.fc_out(trg)
    return out # out : [batch, trg_len, out_dim]

```
<br><br>
## Transformer
![image](https://user-images.githubusercontent.com/100064247/218198004-870f2079-4819-4691-acb8-10c0db44baec.png)


```python
class Transformer(nn.Module):
  def __init__(self,
               src_vocab_size,
               trg_vocab_size,
               src_pad_idx,
               trg_pad_idx,
               embed_size= 256,
               num_layers =3, # in paper,6(encoder, decoder both)
               forward_expansion = 2,
               heads = 8,
               dropout = 0.1,
               device = "cuda",
               max_length = 100): # 128 , 256, 512, .. based on your data
    super(Transformer, self).__init__()


    self.encoder = Encoder(src_vocab_size,
                           embed_size,
                           num_layers,
                           heads,
                           forward_expansion,
                           dropout,
                           device,
                           max_length)

    self.decoder = Decoder(trg_vocab_size,
                           embed_size,
                           num_layers,
                           heads,
                           forward_expansion,
                           dropout,
                           device,
                           max_length)

    self.src_pad_idx = src_pad_idx
    self.trg_pad_idx = trg_pad_idx
    self.device = device

  # make_src_mask: ì†ŒìŠ¤ë¬¸ì¥ì˜ í† í° ë§ˆìŠ¤í¬ê°’ì„ 0ìœ¼ë¡œ 
  def make_src_mask(self, src): # src: [batch, src_len]
    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2) # src_mask: [batch,1,1,src_len]
    return src_mask.to(self.device)

  # make_trg_mask: íƒ€ê²Ÿë¬¸ì¥ì˜ ê° ë‹¤ìŒë‹¨ì–´ë¥¼ ì•Œ ìˆ˜ ì—†ë„ë¡ masking (time_stepë³„ë¡œ)
  def make_trg_mask(self, trg): # trg: [batch, trg_len]
    N, trg_len = trg.shape
    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(
        N, 1, trg_len, trg_len
    ) 
    return trg_mask.to(self.device)

  def forward(self, src, trg):
    src_mask = self.make_src_mask(src)
    trg_mask = self.make_trg_mask(trg)
    enc_out = self.encoder(src, src_mask) # with src mask 
    out = self.decoder(trg, enc_out, trg_mask, src_mask) # put it into decoder
    return out 
```
## Translation task 

### data
#### ko-en sentence pair ( AI Hub í•œêµ­ì–´-ì˜ì–´ ë²ˆì—­ ë§ë­‰ì¹˜ / êµ¬ì–´ì²´ ë°ì´í„° 1 & 2)
- corpus : 115000
- train: 92000
- valid, test: 11500 
- source_vocab_size : 29004
- target_vocab_size : 19736


### train setting
- 21,554,456 trainable parameters

```python
# model 
embed_size = 256
num_layers = 3
num_heads = 8
dropout = 0.2
max_length = 100

# hyperparameters
epochs = 20
batch_size = 256
learning_rate = 5e-4
optimizer = Adam
```

### lr scheduler & warmup steps q  
<img width="823" alt="image" src="https://user-images.githubusercontent.com/100064247/219110427-f0cf848f-3998-440d-973f-71c9c0cb2f98.png"> 

```python
# optimizer.py

class ScheduledOptim:
    def __init__(self, optimizer, warmup_steps):
        self.init_lr = np.power(params['embed_size'], -0.5)
        self.optimizer = optimizer
        self.step_num = 0
        self.warmup_steps = warmup_steps
    
    def step(self):
        self.step_num += 1 # per each step forward 
        lr = self.init_lr * self.get_scale() # scailing lr 
        
        for p in self.optimizer.param_groups: # Add a param group to the Optimizer s param_groups
            p['lr'] = lr
            
        self.optimizer.step()
    
    def zero_grad(self):
        self.optimizer.zero_grad()
    
    def get_scale(self): # scaling lr 
        return np.min([
            np.power(self.step_num, -0.5), # step_num ** -0.5  
            self.step_num * np.power(self.warmup_steps, -1.5)
        ])

# usage
optimizer = ScheduledOptim(torch.optim.Adam(self.model.parameters(),
                                            betas = [0.9, 0.98],
                                            eps = 1e-9), warmup_steps=4000))
                                                        

```



### train result

```
---------------------------------------------------------
| Epoch: 01  | Train Loss:  5.235  | Valid Loss:  4.350
---------------------------------------------------------
| Epoch: 02  | Train Loss:  4.155  | Valid Loss:  3.833
---------------------------------------------------------
| Epoch: 03  | Train Loss:  3.726  | Valid Loss:  3.499
---------------------------------------------------------
| Epoch: 04  | Train Loss:  3.399  | Valid Loss:  3.272
---------------------------------------------------------
                  . . . . 
| Epoch: 19  | Train Loss:  1.478  | Valid Loss:  2.413
---------------------------------------------------------
| Epoch: 20  | Train Loss:  1.427  | Valid Loss:  2.409
---------------------------------------------------------

---------------------------------------------------------
Test Loss: 2.390
---------------------------------------------------------

```
### BLEU score 

```
------------------------------
Total BLEU Score = 15.31
------------------------------
Individual BLEU1 score = 46.40
Individual BLEU2 score = 19.43
Individual BLEU3 score = 10.17
Individual BLEU4 score = 6.00
------------------------------
Cumulative BLEU1 score = 46.40
Cumulative BLEU2 score = 30.03
Cumulative BLEU3 score = 20.93
Cumulative BLEU4 score = 15.31
------------------------------
```

```
[100/1000]
ì˜ˆì¸¡: ['the', 'cell', 'phone', 'has', 'a', 'hit', 'and', 'the', 'internet', '.']
ì •ë‹µ: ['there', 'is', 'a', 'clock', 'between', 'a', 'cell', 'phone', 'and', 'a', 'scissor', '.']
[200/1000]
ì˜ˆì¸¡: ['i', 'do', '<unk>', 'want', 'to', 'meet', 'each', 'other', 'anymore', '.']
ì •ë‹µ: ['i', 'do', "n't", 'want', 'to', 'meet', 'you', 'personally', 'anymore', '.']
[300/1000]
ì˜ˆì¸¡: ['i', 'think', 'it', 'might', 'be', 'meaningful', 'and', '<unk>', '.']
ì •ë‹µ: ['it', 'would', 'be', 'difficult', 'to', 'understand', 'what', 'it', 'means', '.']
[400/1000]
ì˜ˆì¸¡: ['practice', 'for', 'a', 'putt', 'before', 'you', 'start', '.']
ì •ë‹µ: ['take', 'a', 'few', 'practice', 'swings', 'before', 'hitting', 'a', 'ball', '.']
[500/1000]
ì˜ˆì¸¡: ['i', 'need', 'to', 'check', 'the', 'volume', '.']
ì •ë‹µ: ['i', 'have', 'to', 'make', 'a', 'call', 'on', 'a', 'landline', '.']
[600/1000]
ì˜ˆì¸¡: ['it', 'looks', 'like', 'our', 'team', 'leader', 'asked', 'us', 'to', 'come', '.']
ì •ë‹µ: ['it', 'looks', 'like', 'a', 'new', 'bookkeeper', 'will', 'join', 'our', 'team', '.']
[700/1000]
ì˜ˆì¸¡: ['technology', 'gives', 'us', 'a', 'lot', 'easier', 'than', 'our', 'computer', '.']
ì •ë‹µ: ['computer', 'technology', 'has', 'improved', ' ', 'the', 'way', 'to', 'get', 'information', '.']
[800/1000]
ì˜ˆì¸¡: ['i', '<unk>', 'so', 'sorry', ',', 'i', 'did', '<unk>', 'speak', 'wrong', '.']
ì •ë‹µ: ['i', "'m", 'awfully', 'sorry', ',', 'it', 'was', 'wrong', 'of', 'me', '.']
[900/1000]
ì˜ˆì¸¡: ['the', 'answer', 'may', 'be', 'able', 'to', 'answer', 'the', 'question', '.']
ì •ë‹µ: ['the', 'resolution', 'of', 'that', 'question', 'may', 'be', 'difficult', '.']
[1000/1000]
ì˜ˆì¸¡: ['so', 'i', 'want', 'to', 'be', 'a', 'great', 'person', 'as', 'possible', '.']
ì •ë‹µ: ['so', 'i', 'hope', 'to', 'be', 'an', 'aircrew', 'as', 'soon', 'as', 'possible', '.']
```
## Reference
https://arxiv.org/abs/1706.03762    
https://pytorch.org/docs/stable/nn.html      
https://github.com/pytorch/tutorials/blob/0d8c59f0822bffc3b1b3e15d3eeed4e24d2918a0/beginner_source/transformer_tutorial.py
https://github.com/Huffon/pytorch-transformer-kor-eng    
https://github.com/nawnoes/pytorch-transformer     
https://github.com/hyunwoongko/transformer      
https://www.youtube.com/watch?v=U0s0f995w14     
https://www.youtube.com/watch?v=Yk1tV_cXMMU&t=20s     
https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb
